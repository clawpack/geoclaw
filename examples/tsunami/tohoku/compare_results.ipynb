{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:2px\"></hr>\n",
    "\n",
    "# Comparisons between direct data measurements and GeoClaw output.\n",
    "<hr style=\"border-width:2px\"></hr>\n",
    "\n",
    "This notebook reproduces a sample of the results found in the paper\n",
    "\n",
    "M. Arcos and R. J. LeVeque, *Validating Velocities in the GeoClaw Tsunami Model using Observations Near Hawaii from the 2011 Tohoku Tsunami*, Pure and Applied Geophysics, 2015. http://dx.doi.org/10.1007/s00024-014-0980-y\n",
    "\n",
    "To run this notebook, the following are necessary : \n",
    "\n",
    "* **Archived data**.  The direct measurements from current meters and tide gauges and accompanying Python scripts used to produce results in  Arcos/LeVeque can be downloaded from [this archive](http://doi.org/10.5281/zenodo.12185).  Python routines from this archive needed in to reproduce results here are duplicated in this notebook.   \n",
    "\n",
    "\n",
    "* **Numerical gauge output from GeoClaw**. To create plots comparing data with GeoClaw output, the GeoClaw Tohoku example should be run with numerical gauges 1123 and 5680.  For the best comparisions, the GeoClaw example should be run with setting `amr_level_max=6`.  This run takes roughly an hour.  \n",
    "\n",
    "\n",
    "* **The Pandas python module**.  The [Pandas](https://pypi.org/project/pandas/) module is used here to load and post-process current meter and gauge data.  Pandas is particularly useful when handling time and date data.\n",
    "\n",
    "This notebook is run using Anaconda Python, version 3.7.4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observational data downloaded from http://doi.org/10.5281/zenodo.12185\n",
    "obs_dir = os.path.join('rjleveque-tohoku2011-paper2-096e44c')\n",
    "\n",
    "# Directory containing gauge files gauge01123.txt and gauge05680.txt from GeoClaw\n",
    "# To get a good comparison with observational data, the GeoClaw example should be \n",
    "# run with amr_level_max=6.  This takes about an hour to run. \n",
    "geoclaw_outdir = '_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earthquake data\n",
    "\n",
    "The earthquake time is localized in UTC time coordinates.  We convert to Pacific/Honolulu so that it can be subtracted from times provided in gauge files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tquake = pandas.Timestamp('05:46:24 UTC on March 11, 2011',tz='Pacific/Honolulu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines for post-processing data \n",
    "\n",
    "The following routines are applied to rows of Pandas dataframes used to store current meter and tide gauge measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for time since quake (entry type pandas.Timedelta)\n",
    "def time_since_quake(row):\n",
    "    return row['date_time'] - tquake\n",
    "\n",
    "# Add column for hours since quake (entry type : float64)\n",
    "def hours_since_quake(row):\n",
    "    return row['time_since_quake'].value/1e9/3600.\n",
    "\n",
    "# Use this to construct velocities from speed and directions.\n",
    "def set_velocity(row):\n",
    "    s = row['Speed']\n",
    "    theta = row['Dir']\n",
    "    u = s * cos((90.-theta)*pi/180.)\n",
    "    v = s * sin((90.-theta)*pi/180.)\n",
    "    return [u,v]\n",
    "\n",
    "# Compute final average\n",
    "def average_vel(row,n):\n",
    "    return [row['u']/n, row['v']/n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create depth averaged data from current meters \n",
    "\n",
    "Read depth data from depth files in `Observations\\HAI*` directories and average the velocity data from these files to get single depth averaged u and v-velocities.  These depth-averaged velocities, along with date/time data and time since earthquake will be stored as columns in a Pandas DataFrame.   Data frames for each depth location will be stored as entries in a dictionary `gauges_avg`.  Directory keys are directory names containing depth files.  \n",
    "\n",
    "The depth-averaged velocities are then detided and results are stored in text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set columns to use for depth DataFrame\n",
    "cols = ['DATE', 'TIME','Speed', 'Dir']\n",
    "\n",
    "# Data provided from current meters is localized to Pacific/Honolulu time zone.\n",
    "def date_time(row):\n",
    "    ts = row['TIME']\n",
    "    nt = row['DATE'].replace(hour=ts.hour,minute=ts.minute,second=ts.second)\n",
    "    return nt.tz_localize('Pacific/Honolulu')\n",
    "\n",
    "# Create list of directories containing depth files\n",
    "dlist = os.listdir(os.path.join(obs_dir, 'Observations'))\n",
    "depth_dirs = []\n",
    "for d in dlist:\n",
    "    p = os.path.join(obs_dir,'Observations',d)\n",
    "    if (os.path.isdir(p)):\n",
    "        depth_dirs.append(d)\n",
    "\n",
    "# Store data frames for each location here\n",
    "gauges_avg = {}\n",
    "for gdir in depth_dirs:\n",
    "    print(\"Reading data in directory {:s}\".format(gdir))\n",
    "        \n",
    "    # Read in depth file names\n",
    "    depth_filenames = glob.glob(os.path.join(obs_dir,'Observations',gdir,\\\n",
    "                                'depth_*m.txt'))\n",
    "\n",
    "    # ---------------------------------------------------------------------    \n",
    "    # Read first depth file to get time and date info.  This assumes all depth \n",
    "    # files use the same time values (?)\n",
    "    f0 = depth_filenames[0]\n",
    "    df_dates = pandas.read_csv(f0,parse_dates=['DATE','TIME'], \\\n",
    "                               sep='\\s+', \\\n",
    "                               names=cols,comment='#')\n",
    "\n",
    "    # Combine date and time columns to get correct localize date/time stamp\n",
    "    df_dates['date_time'] = df_dates.apply(date_time,axis=1)\n",
    "    \n",
    "    # Subtract time of initial quake to get a 'Timedelta'. \n",
    "    df_dates['time_since_quake'] = df_dates.apply(time_since_quake,axis=1)\n",
    "    \n",
    "    # Convert time delta to numerical value\n",
    "    df_dates['hours_since_quake'] = df_dates.apply(hours_since_quake,axis=1)\n",
    "    \n",
    "    if gdir == \"HAI1123_Kahului_harbor\":\n",
    "        # Correction mentioned in detide.py\n",
    "        #    if gaugeno==1123:\n",
    "        #        t2 = t2 - 1.   # correct error in NGDC data for this gauge\n",
    "        df_dates['hours_since_quake'] -= 1       \n",
    "\n",
    "    # ---------------------------------------------------------------------    \n",
    "    # Create new DataFrame using only date/time info created above\n",
    "    df_dates = df_dates.drop(['DATE','TIME','Speed','Dir'],axis=1)\n",
    "    gf_avg = pandas.DataFrame(df_dates)\n",
    "\n",
    "    # Add velocity columns and initialize velocities to zero.\n",
    "    gf_avg[['u','v']] = gf_avg.apply(lambda row : [0,0],axis=1,result_type='expand')\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Read all files and accumulate velocity averages.\n",
    "    for f in depth_filenames:\n",
    "        # print('Reading {:s}'.format(f))\n",
    "        df = pandas.read_csv(f, parse_dates=['DATE','TIME'], \\\n",
    "                              sep='\\s+',names=cols, \\\n",
    "                              comment='#')    \n",
    "        \n",
    "        # Compute velocity (u,v) from speed and direction stored in depth file\n",
    "        df[['u','v']] = df.apply(set_velocity,axis=1,result_type='expand')\n",
    "    \n",
    "        # Accumulate velocities to be averaged. \n",
    "        gf_avg['u'] += df['u']\n",
    "        gf_avg['v'] += df['v']\n",
    "\n",
    "    # Average velocities\n",
    "    n = len(depth_filenames)   \n",
    "    gf_avg[['u','v']] = gf_avg.apply(average_vel, axis=1, result_type='expand', args=(n,))\n",
    "    \n",
    "    # Store data frame in a dictionary\n",
    "    gauges_avg[gdir] = gf_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display resulting data frame containing averaged values\n",
    "Extract the data frame from dictionary, and display results of averaged velocities.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauges_avg['HAI1123_Kahului_harbor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detide average velocity data using a best-fit polynomial\n",
    "This subroutine was taking from `TG_DART_tools.py` in archive data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tide_poly(t,eta,degree):\n",
    "    \"\"\"\n",
    "    Fit a polynomial of the specified degree to data \n",
    "    Returns the coefficents c of c[0] + c[1]*t + ...\n",
    "    and the polynomial fit eta_fit.\n",
    "    \"\"\"\n",
    "    from numpy.linalg import lstsq, svd\n",
    "    # from pylab import find\n",
    "    \n",
    "\n",
    "    if numpy.any(numpy.isnan(eta)):\n",
    "        eta2 = numpy.where(numpy.isnan(eta), 1e50, eta)\n",
    "        # j_nonan = find(eta2 < 1e40)\n",
    "        # t_nonan = t[j_nonan]\n",
    "        # eta_nonan = eta[j_nonan]\n",
    "        t_nonan = t[eta2 < 1e40]\n",
    "        eta_nonan = eta[eta2 < 1e40]\n",
    "        print(\"Ignoring %i NaN values\" % (len(eta)-len(eta_nonan)))\n",
    "    else:\n",
    "        t_nonan = t\n",
    "        eta_nonan = eta\n",
    "\n",
    "    # Scale data so matrix better conditioned:\n",
    "\n",
    "    scale_factor = abs(t_nonan).max()\n",
    "    t_nonan = t_nonan/scale_factor\n",
    "    t = t/scale_factor\n",
    "\n",
    "    # Use Newton polynomial basis using these points:\n",
    "    tpts = numpy.linspace(t_nonan.min(),t_nonan.max(),degree+1)\n",
    "    \n",
    "    # Form A matrix for least squares fit \n",
    "    A = numpy.ones((len(t_nonan),degree+1))\n",
    "    for j in range(1,degree+1):\n",
    "        A[:,j] = A[:,j-1] * (t_nonan - tpts[j])\n",
    "    ncols = A.shape[1]\n",
    "        \n",
    "    # Perform least squares fit:\n",
    "    c = lstsq(A,eta_nonan)[0]\n",
    "    \n",
    "    if numpy.any(numpy.isnan(eta)):\n",
    "        # Reconstruct A using all times for calculating predicted:\n",
    "        A = numpy.ones((len(t),degree+1))\n",
    "        for j in range(1,degree+1):\n",
    "            A[:,j] = A[:,j-1] * (t - tpts[j])\n",
    "\n",
    "    eta_fit = numpy.dot(A,c)\n",
    "    \n",
    "    return eta_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over all data frames and detide average velocities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in gauges_avg.keys():\n",
    "    gf_avg = gauges_avg[fname]\n",
    "    thours = gf_avg['hours_since_quake']\n",
    "    \n",
    "    u = gf_avg['u'] \n",
    "    v = gf_avg['v'] \n",
    "\n",
    "    degree = 15\n",
    "    u2 = fit_tide_poly(thours,u,degree)\n",
    "    v2 = fit_tide_poly(thours,v,degree)\n",
    "\n",
    "    # These values are essentially what is in \"Observations/HAIXXXX/detided_poly.txt\" \n",
    "    gf_avg['u_detided'] = gf_avg['u'] - u2\n",
    "    gf_avg['v_detided'] = gf_avg['v'] - v2\n",
    "\n",
    "    # Save detided velocities.  These can be compared directly with gauge results from \n",
    "    # GeoClaw.  Time is stored in hours, not seconds, so GeoClaw gauge times\n",
    "    # have to be scaled. \n",
    "    cols = ['hours_since_quake', 'u_detided', 'v_detided']\n",
    "    dname = '{:s}_detided.txt'.format(fname)\n",
    "    print(\"Writing detide file {:s}\".format(dname))\n",
    "    gf_avg.to_csv(dname, columns = cols, \n",
    "                  sep='\\t', \\\n",
    "                  header=False, \\\n",
    "                  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with detide_poly.txt files\n",
    "The file `detide_poly.txt` in `Observations/HAI1123_Kahului_harbor` has detided data used in the Arcos and LeVeque paper.  The first few lines in this file are : \n",
    "        \n",
    "        0.8266666667        -1.2291699317        -0.5807611893\n",
    "        0.9266666667        -4.5312793719        -2.2852610736\n",
    "        1.0266666667        -3.2835789096        -0.8140147343\n",
    "        1.1266666667        -1.4856985873        -0.0352514633\n",
    "        1.2266666667        -3.3774148624        -1.8623566264\n",
    "\n",
    "Comparing these values with values computed above, we see the values are close, but don't match exactly - not sure why.   The solution is quite sensitive to degree of the polynomial used, but less sensitive to the time interval over which to fit the polynomial function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = gauges_avg['HAI1123_Kahului_harbor'][['hours_since_quake','u_detided','v_detided']]\n",
    "t = gf['hours_since_quake']\n",
    "gf[0.82 < t][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results at gauge HAI1123 and compare with GeoClaw\n",
    "\n",
    "The plots below require the numeric gauges computed in the GeoClaw run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(1)\n",
    "clf()\n",
    "\n",
    "gf_avg = gauges_avg['HAI1123_Kahului_harbor']\n",
    "thours = gf_avg['hours_since_quake']\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot reference sea level\n",
    "plot(thours,0*thours,'k-',linewidth=0.5, label='Sea Level')\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot u-velocity data from observations\n",
    "u = gf_avg['u_detided']\n",
    "plot(thours,u,'k.-',markersize=5,label='Observation')\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot u-velocity gauge data from GeoClaw\n",
    "f = os.path.join(geoclaw_outdir,'gauge01123.txt')\n",
    "cols = ['level','t','q[1]','q[2]','q[3]','eta','aux']\n",
    "df_gauge = df_dates = pandas.read_csv(f, sep='\\s+', comment='#', names=cols)\n",
    "\n",
    "tshift = 10*60   # Shift by 10 minutes (mentioned in tohoku paper)\n",
    "tg = (df_gauge['t'] + tshift)/3600 \n",
    "hg = df_gauge['q[1]']\n",
    "ug = 100*df_gauge['q[2]']/hg    # Convert to cm/sec\n",
    "vg = 100*df_gauge['q[3]']/hg    # Convert to cm/sec\n",
    "\n",
    "plot(tg,ug,'r.-',markersize=1, label='GeoClaw')\n",
    "\n",
    "# ------------------------------------------\n",
    "# Fix up axes, add axes labels, title, etc. \n",
    "xlabel('Hours')\n",
    "ylabel('u-velocity (cm/sec)')\n",
    "title('u-velocity (depth averaged)')\n",
    "\n",
    "yticks([-200, -100, 0, 100, 200])\n",
    "ylim([-275,275])\n",
    "xlim([7.25,13])\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(2)\n",
    "clf()\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot v-velocity data from observations\n",
    "gf_avg = gauges_avg['HAI1123_Kahului_harbor']\n",
    "\n",
    "thours = gf_avg['hours_since_quake']\n",
    "v = gf_avg['v_detided']\n",
    "\n",
    "# Plot sea-level\n",
    "plot(thours,0*thours,'k-',linewidth=1,label='Sea Level')\n",
    "\n",
    "# Plot observavational data\n",
    "plot(thours,v,'k.-',markersize=5,label='Observation')\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot v-velocity gauge from GeoClaw\n",
    "plot(tg,vg,'r.-',markersize=1, label='GeoClaw')\n",
    "\n",
    "# ------------------------------------------\n",
    "# Fix axes limits, add title, axes labels etc\n",
    "xlabel('Hours')\n",
    "ylabel('v-velocity (cm/sec)')\n",
    "title('v-velocity (depth averaged)')\n",
    "\n",
    "yticks([-200, -100, 0, 100, 200])\n",
    "ylim([-275,275])\n",
    "xlim([7.5,13])\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "# Read tide gauges and plot surface height\n",
    "\n",
    "In the following, tide gauge data files are read and data used to compare surface elevations with GeoClaw results. \n",
    "\n",
    "Python scripts in archive refer to files `TG_XXXX_raw.csv` in directory `Observations/TideGauges`.  However, the only files that were found are\n",
    "\n",
    "    1615680__2011-03-11_to_2011-03-13.csv\n",
    "    1617760__2011-03-11_to_2011-03-13.csv\n",
    "\n",
    "These are loaded below, and surface height data is compared with results from GeoClaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tide gauge data is localized in UTC coordinates and then converted.\n",
    "def date_time_tides(row):\n",
    "    ts = row['TIME']\n",
    "    nt = row['DATE'].replace(hour=ts.hour,minute=ts.minute,second=ts.second)\n",
    "    return nt.tz_localize('UTC').tz_convert('Pacific/Honolulu')\n",
    "\n",
    "tg_files = []\n",
    "tg_files.append('1615680__2011-03-11_to_2011-03-13.csv')\n",
    "tg_files.append('1617760__2011-03-11_to_2011-03-13.csv')\n",
    "\n",
    "# Store tide gauge data in a dictionary. \n",
    "tide_gauges = {}\n",
    "for k in range(2):\n",
    "    csv_file = os.path.join(obs_dir,'TideGauges',tg_files[k])\n",
    "    print('Reading file {:s}'.format(csv_file.split('/')[-1]))\n",
    "    gf = pandas.read_csv(csv_file,parse_dates=['DATE','TIME'])\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Add column with correct date and time (entry type : pandas.Timestamp)\n",
    "    gf['date_time'] = gf.apply(date_time_tides,axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Add column for time since quake (entry type pandas.Timedelta)\n",
    "    gf['time_since_quake'] = gf.apply(time_since_quake,axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Add column for hours since quake (entry type : float64)\n",
    "    gf['hours_since_quake'] = gf.apply(hours_since_quake,axis=1)\n",
    "\n",
    "    # Clean up values : Set '-' to NaN;  convert numeric types to float\n",
    "    gf = gf.replace('-',nan)\n",
    "    cols = ['1MIN', '6MIN', 'ALTERNATE', 'RESIDUAL', 'PREDICTED']\n",
    "    convert = dict.fromkeys(cols, float)\n",
    "    gf = gf.astype(convert)\n",
    "    \n",
    "    key = tg_files[k].split('__')[0]\n",
    "    tide_gauges[key] = gf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the tide gauge data\n",
    "\n",
    "Display tide gauge data for a single tide gauge.  We drop the `DATE` and `TIME` columns for display purposes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_gauges['1615680'].drop(['DATE','TIME'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detide tide gauge data using harmonic fit\n",
    "\n",
    "The harmonic fit routine belowo was taken from the TG_DART_tools.py module.  The only minor change was to remove `find` function.  It seems this no longer available in pylab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tide_harmonic(t, eta, periods, t0=0, svd_tol=0.01):\n",
    "    from numpy.linalg import lstsq, svd, norm\n",
    "    #from pylab import find\n",
    "    \n",
    "    if numpy.any(numpy.isnan(eta)):\n",
    "        eta2 = numpy.where(numpy.isnan(eta), 1e50, eta)\n",
    "        # j_nonan = find(eta2 < 1e40)\n",
    "        # t_nonan = t[j_nonan]\n",
    "        # eta_nonan = eta[j_nonan]\n",
    "        t_nonan = t[eta2 < 1e40]\n",
    "        eta_nonan = eta[eta2 < 1e40]\n",
    "    else:\n",
    "        t_nonan = t\n",
    "        eta_nonan = eta\n",
    "\n",
    "    A = numpy.ones(t_nonan.shape)\n",
    "    names = periods.keys()\n",
    "    for k in names:\n",
    "        s1 = numpy.sin(2*numpy.pi*t_nonan/periods[k])\n",
    "        c1 = numpy.cos(2*numpy.pi*t_nonan/periods[k])\n",
    "        A = numpy.vstack([A,s1,c1])\n",
    "    A = A.T\n",
    "    ncols = A.shape[1]\n",
    "    \n",
    "    # c,res,rank,s = lstsq(A,eta)  ## Does not work well!\n",
    "    # Using full least squares solution gives very large coefficients\n",
    "    # Instead use SVD based pseudo-inverse throwing away singular\n",
    "    # values below the specified tolerance:\n",
    "\n",
    "    U,S,V = svd(A, full_matrices=False)\n",
    "    #print \"Singular values: \",S\n",
    "    \n",
    "    c = numpy.zeros(ncols)\n",
    "    num_sv = 0\n",
    "    for k in range(ncols):\n",
    "        if S[k]/S[0] > svd_tol:\n",
    "            c = c + (numpy.dot(U[:,k],eta_nonan) / S[k]) * V[k,:]\n",
    "            num_sv += 1\n",
    "    print(\"Inverting using %s singular values out of %s\" % (num_sv, ncols))\n",
    "    \n",
    "    c_sin = c[1::2]\n",
    "    c_cos = c[2::2]\n",
    "    c_cos = numpy.where(abs(c_cos)<1e-10, 1e-10, c_cos)\n",
    "    phi = -numpy.arctan(c_sin / c_cos) * 180./numpy.pi\n",
    "    phi = numpy.where(c_cos < 0., phi+180, phi)\n",
    "    \n",
    "\n",
    "    # Determine offset, amplitude, phase so that fit has the form\n",
    "    #  eta_fit = offset + sum_k amplitude[k] * cos(2*pi*(t - t0) + phase[k])\n",
    "    # where the sum is over all harmonic constituents in periods.keys()\n",
    "\n",
    "    offset = c[0]  # constant term in fit\n",
    "    phase = {}\n",
    "    amplitude = {}\n",
    "    for i,name in enumerate(names):\n",
    "        amplitude[name] = numpy.sqrt(c_sin[i]**2 + c_cos[i]**2)\n",
    "        phase[name] = phi[i] + 360.*t0/periods[name]\n",
    "\n",
    "    #print \"c: \",c\n",
    "    #print \"c_cos: \",c_cos\n",
    "    #print \"c_sin: \",c_sin\n",
    "    #print \"+++ offset, amplitude, phase: \",offset, amplitude, phase\n",
    "        \n",
    "    if numpy.any(numpy.isnan(eta)):\n",
    "        # Reconstruct A using all times for calculating predicted:\n",
    "        A = numpy.ones(t.shape)\n",
    "        for k in names:\n",
    "            s1 = numpy.sin(2*numpy.pi*t/periods[k])\n",
    "            c1 = numpy.cos(2*numpy.pi*t/periods[k])\n",
    "            A = numpy.vstack([A,s1,c1])\n",
    "        A = A.T\n",
    "\n",
    "    eta_fit = numpy.dot(A,c)\n",
    "\n",
    "    # residual = eta - eta_fit\n",
    "    # print \"Norm of residual: \",norm(residual,2)  # might return NaN\n",
    "    # print \"Norm of amplitudes: \",norm(c[1:],2)\n",
    "    \n",
    "    return eta_fit, amplitude, phase, offset\n",
    "\n",
    "def get_periods():\n",
    "    \"\"\"\n",
    "    Returns dictionary of tidal harmonic constituent periods (in hours).\n",
    "    \"\"\"\n",
    "    \n",
    "    periods = { \\\n",
    "        'K1': 23.9344697,\n",
    "        'O1': 25.8193417,\n",
    "        'M2': 12.4206012,\n",
    "        'S2': 12.0000000,\n",
    "        'M3': 08.2804008,\n",
    "        'M4': 06.2103006,\n",
    "        '2MK5': 04.9308802,\n",
    "        'M6': 04.1402004,\n",
    "        '3MK7': 03.10515030,\n",
    "        'M8': 03.1051503,\n",
    "        'N2': 12.6583482,\n",
    "        'Q1': 26.8683567,\n",
    "        'MK3': 08.1771399,\n",
    "        'S4': 06.0000000,\n",
    "        'MN4': 06.2691739,\n",
    "        'NU2': 12.6260044,\n",
    "        'S6': 04.0000000,\n",
    "        'MU2': 12.8717576,\n",
    "        '2N2': 12.9053745,\n",
    "        'OO1': 22.3060742,\n",
    "        'LAM2': 12.2217742,\n",
    "        'S1': 24.0000000,\n",
    "        'M1': 24.8332484,\n",
    "        'J1': 23.0984768,\n",
    "        'MM': 661.3092049,\n",
    "        'SSA': 4382.9052087,\n",
    "        'SA': 8765.8210896,\n",
    "        'MSF': 354.3670522,\n",
    "        'MF': 327.8589689,\n",
    "        'RHO': 26.7230533,\n",
    "        'T2': 12.0164492,\n",
    "        'R2': 11.9835958,\n",
    "        '2Q1': 28.0062225,\n",
    "        'P1': 24.0658902,\n",
    "        '2SM2': 11.6069516,\n",
    "        'L2': 12.1916202,\n",
    "        '2MK3': 08.3863030,\n",
    "        'K2': 11.9672348,\n",
    "        'MS4': 06.1033393,\n",
    "        }\n",
    "    return periods\n",
    "\n",
    "periods = get_periods()\n",
    "constituents_hawaii = ['J1','K1','K2','M2','N2','O1','P1','Q1','S2','SA']\n",
    "periods_hawaii = {k:periods[k] for k in constituents_hawaii}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detide tide gauge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = periods_hawaii\n",
    "\n",
    "for fname in tide_gauges.keys():\n",
    "    gf_detide = tide_gauges[fname]\n",
    "\n",
    "    thours = gf_detide['hours_since_quake']\n",
    "    eta = gf_detide['1MIN']\n",
    "\n",
    "    eta_fit, eta_offset, eta_amplitude, eta_phase= \\\n",
    "                        fit_tide_harmonic(thours, eta, periods=p, t0=0, svd_tol=1e-5)\n",
    "\n",
    "    # Store detided data in DataFrame\n",
    "    gf_detide['eta_detided'] = eta - eta_fit\n",
    "    \n",
    "    # Save detided velocities.  These can be compared directly with gauge results from \n",
    "    # GeoClaw.  Time is stored in hours, not seconds, so GeoClaw gauge times\n",
    "    # have to be scaled. \n",
    "    cols = ['hours_since_quake', 'eta_detided']\n",
    "    dname = '{:s}_detided.txt'.format(fname)\n",
    "    print(\"Writing detide file {:s}\".format(dname))\n",
    "    gf_detide.to_csv(dname, columns = cols, \n",
    "                     sep='\\t', \\\n",
    "                     header=False, \\\n",
    "                     index=False,\\\n",
    "                     na_rep = 'nan')\n",
    "    # savetxt(dname,gf_detide[cols].values,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot surface height data\n",
    "\n",
    "Plot the surface height data from gauge 1614680 and compare to GeoClaw results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(3)\n",
    "clf()\n",
    "\n",
    "gf = tide_gauges['1615680']\n",
    "\n",
    "thours = gf['hours_since_quake']\n",
    "\n",
    "# Plot sea level\n",
    "plot(thours,0*thours,'k-',label='Sea Level', linewidth=0.5)\n",
    "\n",
    "# Plot data from observations\n",
    "plot(thours,gf['eta_detided'],'k.-',markersize=5,label='Observation')\n",
    "\n",
    "# Plot gauge data from GeoClaw\n",
    "f = os.path.join(geoclaw_outdir,'gauge05680.txt')\n",
    "cols = ['level','t','q[1]','q[2]','q[3]','eta','aux']\n",
    "df_gauge = pandas.read_csv(f, sep='\\s+', comment='#', names=cols)\n",
    "tshift = 10*60\n",
    "tg = (df_gauge['t'] + tshift)/3600  # Shift by 10 minutes\n",
    "etag = df_gauge['eta']\n",
    "\n",
    "plot(tg,etag,'r.-',markersize=1,label='GeoClaw')\n",
    "\n",
    "xlabel('Hours')\n",
    "ylabel('Surface height')\n",
    "title('Surface Height (1615680)')\n",
    "\n",
    "yticks(linspace(-2,2,9))\n",
    "xlim([7.5,13])\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
